{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "#import sys\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from math import ceil\n",
    "import util\n",
    "from threading import Thread\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from imblearn.under_sampling import NearMiss\n",
    "#-------------------------------\n",
    "# from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "# from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['n_of_keypoints', 'avg_perc_brightness', 'contrast', 'edge_length1', 'hue1']\n",
    "man_diff = [\n",
    "'ILSVRC2012_val_00000001.JPEG',\n",
    "'ILSVRC2012_val_00000011.JPEG',\n",
    "'ILSVRC2012_val_00000016.JPEG',\n",
    "'ILSVRC2012_val_00000021.JPEG',\n",
    "'ILSVRC2012_val_00000025.JPEG',\n",
    "'ILSVRC2012_val_00000058.JPEG',\n",
    "'ILSVRC2012_val_00000068.JPEG',\n",
    "'ILSVRC2012_val_00000083.JPEG',\n",
    "'ILSVRC2012_val_00000084.JPEG',\n",
    "'ILSVRC2012_val_00000087.JPEG',\n",
    "'ILSVRC2012_val_00000088.JPEG',\n",
    "'ILSVRC2012_val_00000090.JPEG',\n",
    "'ILSVRC2012_val_00000100.JPEG',\n",
    "'ILSVRC2012_val_00000126.JPEG',\n",
    "'ILSVRC2012_val_00000127.JPEG',\n",
    "'ILSVRC2012_val_00000133.JPEG',\n",
    "'ILSVRC2012_val_00000156.JPEG',\n",
    "'ILSVRC2012_val_00000167.JPEG',\n",
    "'ILSVRC2012_val_00000182.JPEG',\n",
    "'ILSVRC2012_val_00000195.JPEG',\n",
    "'ILSVRC2012_val_00000199.JPEG',\n",
    "'ILSVRC2012_val_00000200.JPEG',\n",
    "'ILSVRC2012_val_00000206.JPEG',\n",
    "'ILSVRC2012_val_00000218.JPEG',\n",
    "'ILSVRC2012_val_00000225.JPEG',\n",
    "'ILSVRC2012_val_00000231.JPEG',\n",
    "'ILSVRC2012_val_00000500.JPEG',\n",
    "'ILSVRC2012_val_00000520.JPEG',\n",
    "'ILSVRC2012_val_00000740.JPEG',\n",
    "'ILSVRC2012_val_00000746.JPEG',\n",
    "]\n",
    "nums =[]\n",
    "for x in man_diff:\n",
    "    nums.append(int(x[15:23]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "first_level = []\n",
    "second_level = []\n",
    "third_level = []\n",
    "row_count = 0\n",
    "with open('all_new_features_hier_norm_top_1.csv', 'rb') as csvfile:\n",
    "    lines = [line.decode('utf-8-sig') for line in csvfile]\n",
    "\n",
    "    for row in csv.reader(lines):\n",
    "        # Remove the headers of csv file\n",
    "        if row_count is 0:\n",
    "            row_count = row_count + 1\n",
    "            continue\n",
    "        if row_count in nums:\n",
    "            temp = row[4:7]\n",
    "            temp.append(row[9])\n",
    "            temp.append(row[10])\n",
    "            data.append(temp)                   # changes what features will be used in the premodel. In this array features start at 4th index, and end at the 10th\n",
    "            first_level.append((row[0],row[1]))     # performance of the first level machine\n",
    "            second_level.append((row[0],row[2]))    # performance of the second level machine\n",
    "            third_level.append((row[0],row[3]))     # performance of the third level machine\n",
    "        row_count = row_count + 1\n",
    "        if row_count > 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ILSVRC2012_val_00001000.JPEG',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0.1696358895785899',\n",
       " '-0.22715225702965375',\n",
       " '-0.3267070367443663',\n",
       " '-0.32707747322971165',\n",
       " '-0.1680358998241349',\n",
       " '-0.010860929933379813',\n",
       " '-0.29067509704717026']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(regr_16):\n",
    "    text_representation = tree.export_text(regr_16, feature_names=feature_list)\n",
    "    print('==========CHUNK REPRESENTATION==========')\n",
    "    print(text_representation)\n",
    "#     fig = plt.figure(figsize=(100,80))\n",
    "#     _ = tree.plot_tree(regr_16, feature_names=feature_list, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_fold_worker(test_idx, train_idx, img_data, first_level, second_level, third_level, first_level_machine, second_level_machine, third_level_machine, return_wrapper):\n",
    "    \"\"\"\n",
    "    Worker function for each fold in CV. Trains a model with training data, tests with\n",
    "    test_idx. Places the results as (image, prediction) tuples in return wrapper\n",
    "    Args:\n",
    "        test_idx: List if indexes where the test_data is\n",
    "        train_idx: List if indexes where the train_data is\n",
    "        img_data: all of the image data\n",
    "        first_level: The names of the classes, respective to model return\n",
    "        return_wrapper: The list to add all results\n",
    "    \"\"\"\n",
    "    # Create a validation set which is 10% of the training_data\n",
    "    X_train, _ = util.list_split(img_data, train_idx, [0])\n",
    "    X_train_first_level = X_train\n",
    "    X_train_second_level = X_train\n",
    "    X_train_third_level = X_train\n",
    "\n",
    "\n",
    "    Y_train, _ = util.list_split(img_data, test_idx, [0])\n",
    "    Y_test_first_level, _ = util.list_split(first_level, test_idx, [0])\n",
    "    Y_test_second_level, _ = util.list_split(second_level, test_idx, [0])\n",
    "    Y_test_third_level, _ = util.list_split(third_level, test_idx, [0])\n",
    "\n",
    "    X_test_first_level, _ = util.list_split(first_level, train_idx, [0])\n",
    "    X_test_second_level, _ = util.list_split(second_level, train_idx, [0])\n",
    "    X_test_third_level, _ = util.list_split(third_level, train_idx, [0])\n",
    "\n",
    "    X_val_first_level = [X_test_first_level[i][1] for i in range(0,len(X_test_first_level))]\n",
    "    Y_val_first_level = [Y_test_first_level[i][1] for i in range(0,len(Y_test_first_level))]\n",
    "\n",
    "    X_val_second_level = [X_test_second_level[i][1] for i in range(0,len(X_test_second_level))]\n",
    "    Y_val_second_level = [Y_test_second_level[i][1] for i in range(0,len(Y_test_second_level))]\n",
    "\n",
    "    X_val_third_level = [X_test_third_level[i][1] for i in range(0,len(X_test_third_level))]\n",
    "    Y_val_third_level = [Y_test_third_level[i][1] for i in range(0,len(Y_test_third_level))]\n",
    "\n",
    "    list_predictions = []\n",
    "    Y_train_second_level = []\n",
    "    Y_train_second_level_position = []\n",
    "    Y_train_third_level = []\n",
    "    Y_train_third_level_position = []\n",
    "\n",
    "    ##################################################################################################################\n",
    "    # First Level of hierarchy [Mobilnet_v1]\n",
    "    ##################################################################################################################\n",
    "    if first_level_machine == 'dt16':\n",
    "        predicted_level_2, predicted_level_5, predicted_level_8, predicted_level_12, predicted_level_16 = decision_tree(X_train_first_level, X_val_first_level, Y_train)\n",
    "        predicted = predicted_level_16\n",
    "    for position, prediction in enumerate(predicted):\n",
    "        if first_level_machine == 'dt16':\n",
    "            if prediction > 0.5:\n",
    "                if Y_test_first_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[position][0], 1, prediction, 1, 'tf-mobilenet_v1'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[position][0], 0, prediction, 1, 'tf-mobilenet_v1'))\n",
    "            else:\n",
    "                Y_train_second_level.append(Y_train[position])\n",
    "                Y_train_second_level_position.append(position)\n",
    "        else:\n",
    "            if prediction == 1:\n",
    "                if Y_test_first_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[position][0], 1, prediction, 1, 'tf-mobilenet_v1'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[position][0], 0, prediction, 1, 'tf-mobilenet_v1'))\n",
    "            else:\n",
    "                Y_train_second_level.append(Y_train[position])\n",
    "                Y_train_second_level_position.append(position)\n",
    "\n",
    "    # Not necessary to go to the next level\n",
    "    if len(Y_train_second_level) == 0:\n",
    "        return_wrapper.append(list_predictions)\n",
    "        return\n",
    "\n",
    "    ##################################################################################################################\n",
    "    # Second Level of hierarchy [Inception_v4]\n",
    "    ##################################################################################################################\n",
    "    if second_level_machine == 'dt16':\n",
    "        predicted_level_2, predicted_level_5, predicted_level_8, predicted_level_12, predicted_level_16 = decision_tree(X_train_second_level, X_val_second_level, Y_train_second_level)\n",
    "        predicted = predicted_level_16\n",
    "\n",
    "    for position, prediction in enumerate(predicted):\n",
    "        if second_level_machine == 'dt16':\n",
    "            if prediction > 0.5:\n",
    "                if Y_test_second_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_second_level_position[position]][0], 2, prediction, 2, 'tf-inception_v4'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_second_level_position[position]][0], 0, prediction, 2, 'tf-inception_v4'))\n",
    "            else:\n",
    "                Y_train_third_level.append(Y_train_second_level[position])\n",
    "                Y_train_third_level_position.append(Y_train_second_level_position[position])\n",
    "        else:\n",
    "            if prediction == 1:\n",
    "                if Y_test_second_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_second_level_position[position]][0], 2, prediction, 2, 'tf-inception_v4'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_second_level_position[position]][0], 0, prediction, 2, 'tf-inception_v4'))\n",
    "            else:\n",
    "                Y_train_third_level.append(Y_train_second_level[position])\n",
    "                Y_train_third_level_position.append(Y_train_second_level_position[position])\n",
    "\n",
    "    if len(Y_train_third_level) == 0:\n",
    "        return_wrapper.append(list_predictions)\n",
    "        return\n",
    "\n",
    "    ##################################################################################################################\n",
    "    # Third Level of hierarchy [Resnet_v1_152]\n",
    "    ##################################################################################################################\n",
    "    if third_level_machine == 'dt16':\n",
    "        predicted_level_2, predicted_level_5, predicted_level_8, predicted_level_12, predicted_level_16 = decision_tree(X_train_third_level, X_val_third_level, Y_train_third_level)\n",
    "        predicted = predicted_level_16\n",
    "\n",
    "    for position, prediction in enumerate(predicted):\n",
    "        if third_level_machine == 'dt16':\n",
    "            if prediction > 0.5:\n",
    "                if Y_test_third_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 3, prediction, 3, 'tf-resnet_v1_152'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 0, prediction, 3, 'tf-resnet_v1_152'))\n",
    "            else:\n",
    "                if Y_test_third_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 3, prediction, 0, 'failed'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 0, prediction, 0, 'failed'))\n",
    "        else:\n",
    "            if prediction == 1:\n",
    "                if Y_test_third_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 3, prediction, 3, 'tf-resnet_v1_152'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 0, prediction, 3, 'tf-resnet_v1_152'))\n",
    "            else:\n",
    "                if Y_test_third_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 3, prediction, 0, 'failed'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 0, prediction, 0, 'failed'))\n",
    "\n",
    "\n",
    "    return_wrapper.append(list_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prototype(amount_images, list_premodels):\n",
    "    \"\"\"\n",
    "    Produce a .csv file with the fields <Image_filename, Ground truth model, predicted model>\n",
    "    for every image in the train information set. We use k-fold cross validation, where k=10.\n",
    "    \"\"\"\n",
    "    percentage_results = []\n",
    "    report_results = []\n",
    "\n",
    "    if len(list_premodels) == 0:\n",
    "        print(\"No premodels were selected!\")\n",
    "        return percentage_results\n",
    "    if amount_images == 0:\n",
    "        print(\"No images were selected!\")\n",
    "        return percentage_results\n",
    "\n",
    "    #print(\"Creating training data...\")\n",
    "    #data, first_level_data, second_level_data, third_level_data = self.cv_training_data(amount_images)\n",
    "\n",
    "    for counter,(first_level_machine, second_level_machine, third_level_machine) in enumerate(list_premodels):\n",
    "        # Split training data in k-fold chunks\n",
    "        # Minimum needs to be 2\n",
    "        k_fold = 10\n",
    "        worker_threads = list()\n",
    "        chunk_size = int(ceil(len(data) / float(k_fold)))\n",
    "        # Create a new thread for each fold\n",
    "        for i, (test_idx, train_idx) in enumerate(util.chunkise(range(len(data)), chunk_size)):\n",
    "            return_wrapper = list()\n",
    "            p = Thread(target=CV_fold_worker, args=(test_idx, train_idx, data, first_level, second_level, third_level, first_level_machine, second_level_machine, third_level_machine, return_wrapper))\n",
    "            p.start()\n",
    "            worker_threads.append((p, return_wrapper))\n",
    "\n",
    "\n",
    "        # Wait for threads to finish, collect results\n",
    "        all_predictions = list()\n",
    "        for p, ret_val in worker_threads:\n",
    "            p.join()\n",
    "            all_predictions += ret_val\n",
    "\n",
    "        predicted = []\n",
    "        correct_result = []\n",
    "\n",
    "        for p in all_predictions:\n",
    "            for image, groundtruth_label, result_prediction, prediction, model_predicted in p:\n",
    "                correct_result.append(groundtruth_label)\n",
    "                predicted.append(prediction)\n",
    "\n",
    "        percentage_results.append(accuracy_score(predicted, correct_result, [list_premodels[counter]]))\n",
    "        report_results.append(precision_recall_fscore_support(correct_result, predicted, labels = [0, 1]))\n",
    "    return_wrapper = list()\n",
    "    preds = list()\n",
    "    for counter,(first_level_machine, second_level_machine, third_level_machine) in enumerate(list_premodels):\n",
    "        preds.append(CV_fold_worker(test_idx, train_idx, data, first_level, second_level, third_level, first_level_machine, second_level_machine, third_level_machine, return_wrapper))\n",
    "    return percentage_results, report_results, predicted, correct_result, return_wrapper, worker_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, X_test, Y_train):\n",
    "    \"\"\"\n",
    "    Decision Tree function that returns the prediction of a list of images. This function allows different deepth levels: 2,5,8,12 and 16\n",
    "    Args:\n",
    "        X_train: List of images features used for training\n",
    "        X_test: List of images results used for validate the trained images.\n",
    "        Y_train: List of images features predicted\n",
    "    \"\"\"\n",
    "\n",
    "    # Create tree\n",
    "    regr_2 = DecisionTreeRegressor(max_depth=2)\n",
    "    regr_5 = DecisionTreeRegressor(max_depth=5)\n",
    "    regr_8 = DecisionTreeRegressor(max_depth=8)\n",
    "    regr_12 = DecisionTreeRegressor(max_depth=12)\n",
    "    regr_16 = DecisionTreeRegressor(max_depth=16)\n",
    "\n",
    "    # Fit tree\n",
    "    regr_2.fit(X_train, X_test)\n",
    "    regr_5.fit(X_train, X_test)\n",
    "    regr_8.fit(X_train, X_test)\n",
    "    regr_12.fit(X_train, X_test)\n",
    "    regr_16.fit(X_train, X_test)\n",
    "\n",
    "    # Predict\n",
    "    predicted_level_2 = regr_2.predict(Y_train)\n",
    "    predicted_level_5 = regr_5.predict(Y_train)\n",
    "    predicted_level_8 = regr_8.predict(Y_train)\n",
    "    predicted_level_12 = regr_12.predict(Y_train)\n",
    "    predicted_level_16 = regr_16.predict(Y_train)\n",
    "    visualize(regr_16)\n",
    "\n",
    "    return predicted_level_2, predicted_level_5, predicted_level_8, predicted_level_12, predicted_level_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========CHUNK REPRESENTATION====================CHUNK REPRESENTATION==========\n",
      "\n",
      "|--- n_of_keypoints <= -8.59\n",
      "|   |--- value: [0.00]\n",
      "|--- n_of_keypoints >  -8.59\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- avg_perc_brightness <= -0.44\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- avg_perc_brightness >  -0.44\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- edge_length1 <= -0.31\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "\n",
      "|--- hue1 <= 5.65\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- contrast <= -0.56\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.56\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- hue1 >  5.65\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "==========CHUNK REPRESENTATION==========\n",
      "|--- contrast <= 10.46\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- value: [1.00]\n",
      "|--- contrast >  10.46\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "==========CHUNK REPRESENTATION==========\n",
      "|--- contrast <= -0.53\n",
      "|   |--- edge_length1 <= -0.28\n",
      "|   |   |--- value: [1.00]\n",
      "|   |--- edge_length1 >  -0.28\n",
      "|   |   |--- value: [0.00]\n",
      "|--- contrast >  -0.53\n",
      "|   |--- edge_length1 <= -0.30\n",
      "|   |   |--- edge_length1 <= -0.31\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- edge_length1 >  -0.30\n",
      "|   |   |--- value: [1.00]\n",
      "\n",
      "==========CHUNK REPRESENTATION==========\n",
      "|--- hue1 <= 5.65\n",
      "|   |--- edge_length1 <= -0.30\n",
      "|   |   |--- contrast <= -0.44\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.44\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- edge_length1 >  -0.30\n",
      "|   |   |--- value: [1.00]\n",
      "|--- hue1 >  5.65\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "==========CHUNK REPRESENTATION==========\n",
      "|--- edge_length1 <= 4.41\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- avg_perc_brightness <= -0.44\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- avg_perc_brightness >  -0.44\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- edge_length1 <= -0.31\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- edge_length1 >  4.41\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "==========CHUNK REPRESENTATION==========\n",
      "|--- hue1 <= 5.65\n",
      "|   |--- contrast <= -0.52\n",
      "|   |   |--- contrast <= -0.56\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.56\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.52\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- contrast <= -0.44\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- contrast >  -0.44\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- hue1 >  5.65\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "==========CHUNK REPRESENTATION====================CHUNK REPRESENTATION==========\n",
      "|--- n_of_keypoints <= -6.21\n",
      "|   |--- value: [0.00]\n",
      "|--- n_of_keypoints >  -6.21\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- contrast <= -0.56\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.56\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- contrast <= -0.44\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- contrast >  -0.44\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "\n",
      "\n",
      "|--- edge_length1 <= 4.41\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- edge_length1 >  4.41\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "==========CHUNK REPRESENTATION==========\n",
      "|--- n_of_keypoints <= -8.59\n",
      "|   |--- value: [0.00]\n",
      "|--- n_of_keypoints >  -8.59\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- edge_length1 <= -0.31\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "\n",
      "==========CHUNK REPRESENTATION==========\n",
      "|--- n_of_keypoints <= -8.59\n",
      "|   |--- value: [0.00]\n",
      "|--- n_of_keypoints >  -8.59\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\miniconda3\\envs\\esrg\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass normalize=[('dt16', 'dt16', 'dt16')] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "C:\\Users\\Asus\\miniconda3\\envs\\esrg\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Asus\\miniconda3\\envs\\esrg\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "output_dt16 = prototype(30, ([('dt16', 'dt16', 'dt16')]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0.15081637114302943',\n",
       "  '0.059826081390577976',\n",
       "  '-0.4620720682274576',\n",
       "  '-0.276165537277217',\n",
       "  '-0.17028688337596423'],\n",
       " ['0.17914571948116628',\n",
       "  '-0.43376318321814644',\n",
       "  '-0.46138133349382177',\n",
       "  '-0.3085013703668937',\n",
       "  '-0.30331289750296825'],\n",
       " ['0.17657737470448895',\n",
       "  '-0.4201452801977953',\n",
       "  '-0.46160257748393535',\n",
       "  '-0.18134103521055042',\n",
       "  '-0.007280742254338971'],\n",
       " ['0.1788830372971542',\n",
       "  '-0.3998560123808238',\n",
       "  '-0.489439786769501',\n",
       "  '-0.29380859354074434',\n",
       "  '-0.30234126470866285'],\n",
       " ['0.17852834831154546',\n",
       "  '-0.38572235228189317',\n",
       "  '-0.49501443268173867',\n",
       "  '-0.3011375837129916',\n",
       "  '-0.386056297957872'],\n",
       " ['0.1793671801620258',\n",
       "  '-0.4335617659972259',\n",
       "  '-0.5029713969536185',\n",
       "  '-0.2952394955767403',\n",
       "  '-0.2948780700571947'],\n",
       " ['0.17848053081075577',\n",
       "  '-0.3821636357536901',\n",
       "  '-0.5000401378831645',\n",
       "  '-0.3078186734491865',\n",
       "  '-0.3658817494918784'],\n",
       " ['0.17641447914611616',\n",
       "  '-0.34285307475515164',\n",
       "  '-0.3994069894984872',\n",
       "  '-0.23670340860939695',\n",
       "  '-0.3671198859290685'],\n",
       " ['0.1783113538608788',\n",
       "  '-0.3853407888305528',\n",
       "  '-0.4337613646929737',\n",
       "  '-0.30537563469924073',\n",
       "  '-0.35642330280263107'],\n",
       " ['-12.470942310400751',\n",
       "  '5.996665513665269',\n",
       "  '16.07625548397819',\n",
       "  '5.579407864168465',\n",
       "  '10.86262789984619'],\n",
       " ['0.17919352331033944',\n",
       "  '-0.4247468117136569',\n",
       "  '-0.47024573810039844',\n",
       "  '-0.3030057254310516',\n",
       "  '-0.380467760694468'],\n",
       " ['0.1786971750994283',\n",
       "  '-0.42915121161969666',\n",
       "  '-0.4253851247759062',\n",
       "  '-0.2732597102229613',\n",
       "  '-0.28042665443328024'],\n",
       " ['0.17860989499506333',\n",
       "  '-0.37922407097905686',\n",
       "  '-0.5098703901079497',\n",
       "  '-0.30792319564253096',\n",
       "  '-0.3806996376822809'],\n",
       " ['0.10927192016551007',\n",
       "  '-0.5006645461618475',\n",
       "  '0.4766120932471342',\n",
       "  '-0.04449139260901275',\n",
       "  '-0.42157771513036196'],\n",
       " ['0.1797241447444998',\n",
       "  '-0.43381432338704656',\n",
       "  '-0.482508082013758',\n",
       "  '-0.28871439856547504',\n",
       "  '-0.3821455500206264'],\n",
       " ['0.17918189382390243',\n",
       "  '-0.46568980994900283',\n",
       "  '-0.5764246119547118',\n",
       "  '-0.31189424472306726',\n",
       "  '-0.3892577524198407'],\n",
       " ['0.17706119039603388',\n",
       "  '-0.41273082056391747',\n",
       "  '-0.5345560046149612',\n",
       "  '-0.2408043314873142',\n",
       "  '-0.3702326957070205'],\n",
       " ['0.17475276218552696',\n",
       "  '-0.4555309029303412',\n",
       "  '-0.3989303508354585',\n",
       "  '-0.17209523750247038',\n",
       "  '0.12499424399141582'],\n",
       " ['0.17751921197150528',\n",
       "  '-0.42746801134729184',\n",
       "  '-0.3638513428188342',\n",
       "  '-0.2051487544498595',\n",
       "  '-0.3650721606105008'],\n",
       " ['0.1775901451384216',\n",
       "  '-0.3791620425115007',\n",
       "  '-0.4465998202509347',\n",
       "  '-0.24534032697400945',\n",
       "  '-0.37684322304524753'],\n",
       " ['-4.701160050168433',\n",
       "  '7.381162413258869',\n",
       "  '4.852835434272718',\n",
       "  '1.4491769572625823',\n",
       "  '0.4376566541788143'],\n",
       " ['0.17840994207037136',\n",
       "  '-0.3786923474338046',\n",
       "  '-0.4417018911429636',\n",
       "  '-0.3100391602510744',\n",
       "  '-0.3332643930965992'],\n",
       " ['0.17902484682174918',\n",
       "  '-0.45362858833305625',\n",
       "  '-0.39699481071090775',\n",
       "  '-0.2902192301421839',\n",
       "  '-0.2887273228965664'],\n",
       " ['0.16982800421505517',\n",
       "  '-0.464750938241857',\n",
       "  '-0.509082067218075',\n",
       "  '-0.30156941343215304',\n",
       "  '-0.3905330223170353'],\n",
       " ['0.04349993979272944',\n",
       "  '0.672764127470633',\n",
       "  '0.3461778753563597',\n",
       "  '3.247078990253557',\n",
       "  '-0.2823239259874375'],\n",
       " ['0.05562334930855796',\n",
       "  '0.6159116692552662',\n",
       "  '0.4061591418368474',\n",
       "  '0.1609331896290822',\n",
       "  '-0.11407600440936441'],\n",
       " ['0.1768766022110152',\n",
       "  '-0.457841454080037',\n",
       "  '-0.5213888674198685',\n",
       "  '-0.2775955993673566',\n",
       "  '-0.38874143812001033'],\n",
       " ['0.14287413657042825',\n",
       "  '-0.4696359185358339',\n",
       "  '-0.4493074662786664',\n",
       "  '-0.3021384740259642',\n",
       "  '-0.38289846508741193'],\n",
       " ['0.17238930793212995',\n",
       "  '-0.4897851396853524',\n",
       "  '-0.47681793570452247',\n",
       "  '-0.2570700546505947',\n",
       "  '-0.390099354246235'],\n",
       " ['0.17010973510449592',\n",
       "  '-0.3559985773058515',\n",
       "  '-0.5044154940621909',\n",
       "  '-0.29511472579957665',\n",
       "  '-0.389775452942516']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "def img_num_from_name(img_name):          # obtaining the image number from image name\n",
    "    ext = []\n",
    "    for i in range(len(img_name)):\n",
    "        if i > 14 and i < 23:\n",
    "            ext.append(img_name[i])\n",
    "    j = 0\n",
    "    for i in range(len(ext)):\n",
    "        if ext[i] == '0':\n",
    "            j = j + 1\n",
    "        if ext[i] != '0':\n",
    "            break\n",
    "    rightNum = []\n",
    "    for i in range(j, len(ext)):\n",
    "        rightNum.append(ext[i])\n",
    "    rightNum = ''.join(rightNum)\n",
    "    rightNum = int(rightNum)\n",
    "    return rightNum\n",
    "\n",
    "predicted_models_dt16 = []                                    # models predicted by dt16 premodel            \n",
    "for i in range(10):\n",
    "    for j in range(len(output_dt16[5][i][1][0])):\n",
    "        img_paths = (output_dt16[5][i][1][0][j][0])\n",
    "        img_nums = (img_num_from_name(img_paths))\n",
    "        if output_dt16[5][i][1][0][j][4] == 'tf-mobilenet_v1' and output_dt16[5][i][1][0][j][3] == 1:\n",
    "            predicted_models_dt16.append([img_nums, 'mobilenet_v1'])\n",
    "        elif output_dt16[5][i][1][0][j][4] == 'tf-inception_v4' and output_dt16[5][i][1][0][j][3] == 2:\n",
    "            predicted_models_dt16.append([img_nums, 'inception_v4'])\n",
    "        elif output_dt16[5][i][1][0][j][4] == 'tf-resnet_v1_152' and output_dt16[5][i][1][0][j][3] == 3:\n",
    "            predicted_models_dt16.append([img_nums, 'resnet_v1_152'])\n",
    "        elif output_dt16[5][i][1][0][j][4] == 'failed':\n",
    "            predicted_models_dt16.append([img_nums, 'failed'])\n",
    "print(len(predicted_models_dt16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'mobilenet_v1'],\n",
       " [11, 'mobilenet_v1'],\n",
       " [16, 'mobilenet_v1'],\n",
       " [21, 'mobilenet_v1'],\n",
       " [25, 'mobilenet_v1'],\n",
       " [58, 'mobilenet_v1'],\n",
       " [68, 'mobilenet_v1'],\n",
       " [83, 'mobilenet_v1'],\n",
       " [84, 'mobilenet_v1'],\n",
       " [87, 'mobilenet_v1'],\n",
       " [88, 'mobilenet_v1'],\n",
       " [90, 'mobilenet_v1'],\n",
       " [100, 'mobilenet_v1'],\n",
       " [126, 'mobilenet_v1'],\n",
       " [127, 'mobilenet_v1'],\n",
       " [133, 'mobilenet_v1'],\n",
       " [156, 'mobilenet_v1'],\n",
       " [167, 'mobilenet_v1'],\n",
       " [182, 'mobilenet_v1'],\n",
       " [195, 'mobilenet_v1'],\n",
       " [199, 'mobilenet_v1'],\n",
       " [200, 'mobilenet_v1'],\n",
       " [206, 'mobilenet_v1'],\n",
       " [218, 'mobilenet_v1'],\n",
       " [225, 'mobilenet_v1'],\n",
       " [231, 'mobilenet_v1'],\n",
       " [500, 'mobilenet_v1'],\n",
       " [520, 'mobilenet_v1'],\n",
       " [740, 'mobilenet_v1'],\n",
       " [746, 'mobilenet_v1']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_models_dt16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT16 premodel accuracy = 0.9\n",
      "DT16 premodel average inference time = 214.20882\n"
     ]
    }
   ],
   "source": [
    "import database\n",
    "n = 1\n",
    "accuracy_premodel = 0\n",
    "time_premodel = 0\n",
    "times = 0\n",
    "dt16_premodel_overhead = 43.00882                                           # average dt16 premodel overhead\n",
    "\n",
    "for i in range(len(predicted_models_dt16)):\n",
    "    if predicted_models_dt16[i][1] != 'failed':\n",
    "        times = times + 1                           # get_model_top_n (next line), where n is 1 or 5\n",
    "        accuracy_premodel = accuracy_premodel + (database.get_model_top_n(\"inference\", predicted_models_dt16[i][0], predicted_models_dt16[i][1], n))\n",
    "        time_premodel = time_premodel + (database.get_model_time(\"inference\", predicted_models_dt16[i][0], predicted_models_dt16[i][1]))\n",
    "\n",
    "dt16_accuracy = accuracy_premodel/len(predicted_models_dt16)                        # dt16 premodel accuracy\n",
    "dt16_time = (time_premodel/times) + dt16_premodel_overhead                             # dt16 premodel inference time\n",
    "print(\"DT16 premodel accuracy = {}\".format(dt16_accuracy))\n",
    "print(\"DT16 premodel average inference time = {}\".format(dt16_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21aee3c9a90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_names = ['DT16']\n",
    "accuracy = [dt16_accuracy]\n",
    "\n",
    "#model_names = ['Mobilenet', 'Inception', 'Resnet', 'LR', 'KNN', 'DT16', 'NB']\n",
    "# accuracy = [mobilenet_right/len(img_nums), inception_right/len(img_nums),\n",
    "#             resnet_right/len(img_nums), log_reg_accuracy, knn_accuracy, dt16_accuracy, nb_accuracy]\n",
    "\n",
    "for i in range(len(accuracy)):\n",
    "    accuracy[i] = accuracy[i]*100\n",
    "\n",
    "ypos = np.arange(len(model_names))\n",
    "\n",
    "plt.xticks(ypos, model_names)\n",
    "plt.ylabel(\"accuracy (%)\")\n",
    "plt.title(\"{} accuracy\".format(\"Top-1\"))\n",
    "plt.bar(ypos, accuracy)\n",
    "plt.ylim(top = 100)\n",
    "plt.ylim(bottom = 20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21aee700a58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "times = [dt16_time]\n",
    "times_premodel = [dt16_premodel_overhead]\n",
    "\n",
    "# times = [mobilenet_time/len(img_nums), inception_time/len(img_nums),\n",
    "#          resnet_time/len(img_nums), log_reg_time, knn_time, dt16_time, nb_time]\n",
    "# times_premodel = [0, 0, 0, log_reg_premodel_overhead, knn_premodel_overhead, dt16_premodel_overhead, nb_premodel_overhead]\n",
    "\n",
    "for i in range(len(times)):\n",
    "    times[i] = times[i]/1000\n",
    "\n",
    "for i in range(len(times_premodel)):\n",
    "    times_premodel[i] = times_premodel[i]/1000\n",
    "    \n",
    "plt.xticks(ypos, model_names)\n",
    "plt.ylabel(\"inference time (s)\")\n",
    "plt.title(\"Inference time\")\n",
    "plt.bar(ypos, times, label = \"Inference\")\n",
    "plt.bar(ypos, times_premodel, label = \"Premodel\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esrg",
   "language": "python",
   "name": "esrg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
