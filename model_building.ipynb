{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "#import sys\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from math import ceil\n",
    "import util\n",
    "from threading import Thread\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from imblearn.under_sampling import NearMiss\n",
    "#-------------------------------\n",
    "# from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "# from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['n_of_keypoints', 'avg_perc_brightness', 'contrast', 'edge_length1', 'hue1']\n",
    "man_diff = [\n",
    "'ILSVRC2012_val_00000001.JPEG',\n",
    "'ILSVRC2012_val_00000011.JPEG',\n",
    "'ILSVRC2012_val_00000016.JPEG',\n",
    "'ILSVRC2012_val_00000021.JPEG',\n",
    "'ILSVRC2012_val_00000025.JPEG',\n",
    "'ILSVRC2012_val_00000058.JPEG',\n",
    "'ILSVRC2012_val_00000068.JPEG',\n",
    "'ILSVRC2012_val_00000083.JPEG',\n",
    "'ILSVRC2012_val_00000084.JPEG',\n",
    "'ILSVRC2012_val_00000087.JPEG',\n",
    "'ILSVRC2012_val_00000088.JPEG',\n",
    "'ILSVRC2012_val_00000090.JPEG',\n",
    "'ILSVRC2012_val_00000100.JPEG',\n",
    "'ILSVRC2012_val_00000126.JPEG',\n",
    "'ILSVRC2012_val_00000127.JPEG',\n",
    "'ILSVRC2012_val_00000133.JPEG',\n",
    "'ILSVRC2012_val_00000156.JPEG',\n",
    "'ILSVRC2012_val_00000167.JPEG',\n",
    "'ILSVRC2012_val_00000182.JPEG',\n",
    "'ILSVRC2012_val_00000195.JPEG',\n",
    "'ILSVRC2012_val_00000199.JPEG',\n",
    "'ILSVRC2012_val_00000200.JPEG',\n",
    "'ILSVRC2012_val_00000206.JPEG',\n",
    "'ILSVRC2012_val_00000218.JPEG',\n",
    "'ILSVRC2012_val_00000225.JPEG',\n",
    "'ILSVRC2012_val_00000231.JPEG',\n",
    "'ILSVRC2012_val_00000500.JPEG',\n",
    "'ILSVRC2012_val_00000520.JPEG',\n",
    "'ILSVRC2012_val_00000740.JPEG',\n",
    "'ILSVRC2012_val_00000746.JPEG',\n",
    "]\n",
    "nums =[]\n",
    "for x in man_diff:\n",
    "    nums.append(int(x[15:23]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "first_level = []\n",
    "second_level = []\n",
    "third_level = []\n",
    "row_count = 0\n",
    "with open('all_new_features_hier_norm_top_1.csv', 'rb') as csvfile:\n",
    "    lines = [line.decode('utf-8-sig') for line in csvfile]\n",
    "\n",
    "    for row in csv.reader(lines):\n",
    "        # Remove the headers of csv file\n",
    "        if row_count is 0:\n",
    "            row_count = row_count + 1\n",
    "            continue\n",
    "        if row_count in nums:\n",
    "            temp = row[4:7]\n",
    "            temp.append(row[9])\n",
    "            temp.append(row[10])\n",
    "            data.append(temp)                   # changes what features will be used in the premodel. In this array features start at 4th index, and end at the 10th\n",
    "            first_level.append((row[0],row[1]))     # performance of the first level machine\n",
    "            second_level.append((row[0],row[2]))    # performance of the second level machine\n",
    "            third_level.append((row[0],row[3]))     # performance of the third level machine\n",
    "        row_count = row_count + 1\n",
    "        if row_count > 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ILSVRC2012_val_00001000.JPEG',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0.1696358895785899',\n",
       " '-0.22715225702965375',\n",
       " '-0.3267070367443663',\n",
       " '-0.32707747322971165',\n",
       " '-0.1680358998241349',\n",
       " '-0.010860929933379813',\n",
       " '-0.29067509704717026']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(regr_16):\n",
    "    text_representation = tree.export_text(regr_16, feature_names=feature_list)\n",
    "    print(text_representation)\n",
    "#     fig = plt.figure(figsize=(100,80))\n",
    "#     _ = tree.plot_tree(regr_16, feature_names=feature_list, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_fold_worker(test_idx, train_idx, img_data, first_level, second_level, third_level, first_level_machine, second_level_machine, third_level_machine, return_wrapper):\n",
    "    \"\"\"\n",
    "    Worker function for each fold in CV. Trains a model with training data, tests with\n",
    "    test_idx. Places the results as (image, prediction) tuples in return wrapper\n",
    "    Args:\n",
    "        test_idx: List if indexes where the test_data is\n",
    "        train_idx: List if indexes where the train_data is\n",
    "        img_data: all of the image data\n",
    "        first_level: The names of the classes, respective to model return\n",
    "        return_wrapper: The list to add all results\n",
    "    \"\"\"\n",
    "    # Create a validation set which is 10% of the training_data\n",
    "    X_train, _ = util.list_split(img_data, train_idx, [0])\n",
    "    X_train_first_level = X_train\n",
    "    X_train_second_level = X_train\n",
    "    X_train_third_level = X_train\n",
    "\n",
    "\n",
    "    Y_train, _ = util.list_split(img_data, test_idx, [0])\n",
    "    Y_test_first_level, _ = util.list_split(first_level, test_idx, [0])\n",
    "    Y_test_second_level, _ = util.list_split(second_level, test_idx, [0])\n",
    "    Y_test_third_level, _ = util.list_split(third_level, test_idx, [0])\n",
    "\n",
    "    X_test_first_level, _ = util.list_split(first_level, train_idx, [0])\n",
    "    X_test_second_level, _ = util.list_split(second_level, train_idx, [0])\n",
    "    X_test_third_level, _ = util.list_split(third_level, train_idx, [0])\n",
    "\n",
    "    X_val_first_level = [X_test_first_level[i][1] for i in range(0,len(X_test_first_level))]\n",
    "    Y_val_first_level = [Y_test_first_level[i][1] for i in range(0,len(Y_test_first_level))]\n",
    "\n",
    "    X_val_second_level = [X_test_second_level[i][1] for i in range(0,len(X_test_second_level))]\n",
    "    Y_val_second_level = [Y_test_second_level[i][1] for i in range(0,len(Y_test_second_level))]\n",
    "\n",
    "    X_val_third_level = [X_test_third_level[i][1] for i in range(0,len(X_test_third_level))]\n",
    "    Y_val_third_level = [Y_test_third_level[i][1] for i in range(0,len(Y_test_third_level))]\n",
    "\n",
    "    list_predictions = []\n",
    "    Y_train_second_level = []\n",
    "    Y_train_second_level_position = []\n",
    "    Y_train_third_level = []\n",
    "    Y_train_third_level_position = []\n",
    "\n",
    "    ##################################################################################################################\n",
    "    # First Level of hierarchy [Mobilnet_v1]\n",
    "    ##################################################################################################################\n",
    "    if first_level_machine == 'dt16':\n",
    "        predicted_level_2, predicted_level_5, predicted_level_8, predicted_level_12, predicted_level_16 = decision_tree(X_train_first_level, X_val_first_level, Y_train)\n",
    "        predicted = predicted_level_16\n",
    "    for position, prediction in enumerate(predicted):\n",
    "        if first_level_machine == 'dt16':\n",
    "            if prediction > 0.5:\n",
    "                if Y_test_first_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[position][0], 1, prediction, 1, 'tf-mobilenet_v1'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[position][0], 0, prediction, 1, 'tf-mobilenet_v1'))\n",
    "            else:\n",
    "                Y_train_second_level.append(Y_train[position])\n",
    "                Y_train_second_level_position.append(position)\n",
    "        else:\n",
    "            if prediction == 1:\n",
    "                if Y_test_first_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[position][0], 1, prediction, 1, 'tf-mobilenet_v1'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[position][0], 0, prediction, 1, 'tf-mobilenet_v1'))\n",
    "            else:\n",
    "                Y_train_second_level.append(Y_train[position])\n",
    "                Y_train_second_level_position.append(position)\n",
    "\n",
    "    # Not necessary to go to the next level\n",
    "    if len(Y_train_second_level) == 0:\n",
    "        return_wrapper.append(list_predictions)\n",
    "        return\n",
    "\n",
    "    ##################################################################################################################\n",
    "    # Second Level of hierarchy [Inception_v4]\n",
    "    ##################################################################################################################\n",
    "    if second_level_machine == 'dt16':\n",
    "        predicted_level_2, predicted_level_5, predicted_level_8, predicted_level_12, predicted_level_16 = decision_tree(X_train_second_level, X_val_second_level, Y_train_second_level)\n",
    "        predicted = predicted_level_16\n",
    "\n",
    "    for position, prediction in enumerate(predicted):\n",
    "        if second_level_machine == 'dt16':\n",
    "            if prediction > 0.5:\n",
    "                if Y_test_second_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_second_level_position[position]][0], 2, prediction, 2, 'tf-inception_v4'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_second_level_position[position]][0], 0, prediction, 2, 'tf-inception_v4'))\n",
    "            else:\n",
    "                Y_train_third_level.append(Y_train_second_level[position])\n",
    "                Y_train_third_level_position.append(Y_train_second_level_position[position])\n",
    "        else:\n",
    "            if prediction == 1:\n",
    "                if Y_test_second_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_second_level_position[position]][0], 2, prediction, 2, 'tf-inception_v4'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_second_level_position[position]][0], 0, prediction, 2, 'tf-inception_v4'))\n",
    "            else:\n",
    "                Y_train_third_level.append(Y_train_second_level[position])\n",
    "                Y_train_third_level_position.append(Y_train_second_level_position[position])\n",
    "\n",
    "    if len(Y_train_third_level) == 0:\n",
    "        return_wrapper.append(list_predictions)\n",
    "        return\n",
    "\n",
    "    ##################################################################################################################\n",
    "    # Third Level of hierarchy [Resnet_v1_152]\n",
    "    ##################################################################################################################\n",
    "    if third_level_machine == 'dt16':\n",
    "        predicted_level_2, predicted_level_5, predicted_level_8, predicted_level_12, predicted_level_16 = decision_tree(X_train_third_level, X_val_third_level, Y_train_third_level)\n",
    "        predicted = predicted_level_16\n",
    "\n",
    "    for position, prediction in enumerate(predicted):\n",
    "        if third_level_machine == 'dt16':\n",
    "            if prediction > 0.5:\n",
    "                if Y_test_third_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 3, prediction, 3, 'tf-resnet_v1_152'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 0, prediction, 3, 'tf-resnet_v1_152'))\n",
    "            else:\n",
    "                if Y_test_third_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 3, prediction, 0, 'failed'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 0, prediction, 0, 'failed'))\n",
    "        else:\n",
    "            if prediction == 1:\n",
    "                if Y_test_third_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 3, prediction, 3, 'tf-resnet_v1_152'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 0, prediction, 3, 'tf-resnet_v1_152'))\n",
    "            else:\n",
    "                if Y_test_third_level[position][1] == 1:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 3, prediction, 0, 'failed'))\n",
    "                else:\n",
    "                    list_predictions.append((Y_test_first_level[Y_train_third_level_position[position]][0], 0, prediction, 0, 'failed'))\n",
    "\n",
    "\n",
    "    return_wrapper.append(list_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prototype(amount_images, list_premodels):\n",
    "    \"\"\"\n",
    "    Produce a .csv file with the fields <Image_filename, Ground truth model, predicted model>\n",
    "    for every image in the train information set. We use k-fold cross validation, where k=10.\n",
    "    \"\"\"\n",
    "    percentage_results = []\n",
    "    report_results = []\n",
    "\n",
    "    if len(list_premodels) == 0:\n",
    "        print(\"No premodels were selected!\")\n",
    "        return percentage_results\n",
    "    if amount_images == 0:\n",
    "        print(\"No images were selected!\")\n",
    "        return percentage_results\n",
    "\n",
    "    #print(\"Creating training data...\")\n",
    "    #data, first_level_data, second_level_data, third_level_data = self.cv_training_data(amount_images)\n",
    "\n",
    "    for counter,(first_level_machine, second_level_machine, third_level_machine) in enumerate(list_premodels):\n",
    "        # Split training data in k-fold chunks\n",
    "        # Minimum needs to be 2\n",
    "        k_fold = 10\n",
    "        worker_threads = list()\n",
    "        chunk_size = int(ceil(len(data) / float(k_fold)))\n",
    "        # Create a new thread for each fold\n",
    "        for i, (test_idx, train_idx) in enumerate(util.chunkise(range(len(data)), chunk_size)):\n",
    "            return_wrapper = list()\n",
    "            p = Thread(target=CV_fold_worker, args=(test_idx, train_idx, data, first_level, second_level, third_level, first_level_machine, second_level_machine, third_level_machine, return_wrapper))\n",
    "            p.start()\n",
    "            worker_threads.append((p, return_wrapper))\n",
    "\n",
    "\n",
    "        # Wait for threads to finish, collect results\n",
    "        all_predictions = list()\n",
    "        for p, ret_val in worker_threads:\n",
    "            p.join()\n",
    "            all_predictions += ret_val\n",
    "\n",
    "        predicted = []\n",
    "        correct_result = []\n",
    "\n",
    "        for p in all_predictions:\n",
    "            for image, groundtruth_label, result_prediction, prediction, model_predicted in p:\n",
    "                correct_result.append(groundtruth_label)\n",
    "                predicted.append(prediction)\n",
    "\n",
    "        percentage_results.append(accuracy_score(predicted, correct_result, [list_premodels[counter]]))\n",
    "        report_results.append(precision_recall_fscore_support(correct_result, predicted, labels = [0, 1]))\n",
    "    return_wrapper = list()\n",
    "    preds = list()\n",
    "    for counter,(first_level_machine, second_level_machine, third_level_machine) in enumerate(list_premodels):\n",
    "        preds.append(CV_fold_worker(test_idx, train_idx, data, first_level, second_level, third_level, first_level_machine, second_level_machine, third_level_machine, return_wrapper))\n",
    "    return percentage_results, report_results, predicted, correct_result, return_wrapper, worker_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, X_test, Y_train):\n",
    "    \"\"\"\n",
    "    Decision Tree function that returns the prediction of a list of images. This function allows different deepth levels: 2,5,8,12 and 16\n",
    "    Args:\n",
    "        X_train: List of images features used for training\n",
    "        X_test: List of images results used for validate the trained images.\n",
    "        Y_train: List of images features predicted\n",
    "    \"\"\"\n",
    "\n",
    "    # Create tree\n",
    "    regr_2 = DecisionTreeRegressor(max_depth=2)\n",
    "    regr_5 = DecisionTreeRegressor(max_depth=5)\n",
    "    regr_8 = DecisionTreeRegressor(max_depth=8)\n",
    "    regr_12 = DecisionTreeRegressor(max_depth=12)\n",
    "    regr_16 = DecisionTreeRegressor(max_depth=16)\n",
    "\n",
    "    # Fit tree\n",
    "    regr_2.fit(X_train, X_test)\n",
    "    regr_5.fit(X_train, X_test)\n",
    "    regr_8.fit(X_train, X_test)\n",
    "    regr_12.fit(X_train, X_test)\n",
    "    regr_16.fit(X_train, X_test)\n",
    "\n",
    "    # Predict\n",
    "    predicted_level_2 = regr_2.predict(Y_train)\n",
    "    predicted_level_5 = regr_5.predict(Y_train)\n",
    "    predicted_level_8 = regr_8.predict(Y_train)\n",
    "    predicted_level_12 = regr_12.predict(Y_train)\n",
    "    predicted_level_16 = regr_16.predict(Y_train)\n",
    "    visualize(regr_16)\n",
    "\n",
    "    return predicted_level_2, predicted_level_5, predicted_level_8, predicted_level_12, predicted_level_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- contrast <= 10.46\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- hue1 <= -0.38\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- hue1 >  -0.38\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- edge_length1 <= -0.31\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- contrast >  10.46\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "|--- contrast <= 10.46\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- hue1 <= -0.38\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- hue1 >  -0.38\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- value: [1.00]\n",
      "|--- contrast >  10.46\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "|--- contrast <= -0.53\n",
      "|   |--- hue1 <= -0.38\n",
      "|   |   |--- value: [1.00]\n",
      "|   |--- hue1 >  -0.38\n",
      "|   |   |--- value: [0.00]\n",
      "|--- contrast >  -0.53\n",
      "|   |--- edge_length1 <= -0.30\n",
      "|   |   |--- contrast <= -0.44\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.44\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- edge_length1 >  -0.30\n",
      "|   |   |--- value: [1.00]\n",
      "\n",
      "|--- contrast <= 10.46\n",
      "|   |--- contrast <= -0.52\n",
      "|   |   |--- contrast <= -0.56\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.56\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.52\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- contrast >  10.46\n",
      "|   |--- value: [0.00]\n",
      "|--- hue1 <= 5.49\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- contrast <= -0.56\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.56\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- edge_length1 <= -0.31\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- hue1 >  5.49\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.56\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|--- n_of_keypoints <= -8.59\n",
      "|   |--- value: [0.00]\n",
      "|--- n_of_keypoints >  -8.59\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- edge_length1 <= -0.31\n",
      "|   |--- edge_length1 <= -0.30\n",
      "|   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- contrast >  10.46\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.56\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|--- n_of_keypoints <= -8.59\n",
      "|   |--- value: [0.00]\n",
      "|--- n_of_keypoints >  -8.59\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- edge_length1 <= -0.31\n",
      "|   |--- edge_length1 <= -0.30\n",
      "|   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- contrast >  10.46\n",
      "|   |--- value: [0.00]\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- contrast <= -0.56\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.56\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- avg_perc_brightness <= -0.38\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |   |--- avg_perc_brightness >  -0.38\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- contrast >  10.46\n",
      "|   |--- value: [0.00]\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.56\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|--- n_of_keypoints <= -8.59\n",
      "|   |--- value: [0.00]\n",
      "|--- n_of_keypoints >  -8.59\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- edge_length1 <= -0.31\n",
      "|   |--- edge_length1 <= -0.30\n",
      "|   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- contrast >  10.46\n",
      "|   |--- value: [0.00]\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- contrast <= -0.56\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |   |--- contrast >  -0.56\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- avg_perc_brightness <= -0.38\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |   |--- avg_perc_brightness >  -0.38\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- contrast >  10.46\n",
      "|   |--- value: [0.00]\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |--- edge_length1 >  -0.30\n",
      "|   |   |--- value: [1.00]\n",
      "\n",
      "\n",
      "|--- value: [0.00]\n",
      "\n",
      "|--- edge_length1 <= -0.30\n",
      "|   |--- n_of_keypoints <= 0.18\n",
      "|   |   |--- value: [1.00]\n",
      "|   |--- n_of_keypoints >  0.18\n",
      "|   |   |--- value: [0.00]\n",
      "|--- edge_length1 >  -0.30\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "|--- edge_length1 <= 4.41\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- edge_length1 <= -0.31\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- edge_length1 >  4.41\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "|--- hue1 <= 5.65\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- edge_length1 <= -0.31\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- hue1 >  5.65\n",
      "|   |--- value: [0.00]\n",
      "\n",
      "|--- hue1 <= 5.65\n",
      "|   |--- contrast <= -0.53\n",
      "|   |   |--- n_of_keypoints <= 0.18\n",
      "|   |   |   |--- value: [0.00]\n",
      "|   |   |--- n_of_keypoints >  0.18\n",
      "|   |   |   |--- value: [1.00]\n",
      "|   |--- contrast >  -0.53\n",
      "|   |   |--- edge_length1 <= -0.30\n",
      "|   |   |   |--- edge_length1 <= -0.31\n",
      "|   |   |   |   |--- value: [1.00]\n",
      "|   |   |   |--- edge_length1 >  -0.31\n",
      "|   |   |   |   |--- value: [0.00]\n",
      "|   |   |--- edge_length1 >  -0.30\n",
      "|   |   |   |--- value: [1.00]\n",
      "|--- hue1 >  5.65\n",
      "|   |--- value: [0.00]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\miniconda3\\envs\\esrg\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass normalize=[('dt16', 'dt16', 'dt16')] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "C:\\Users\\Asus\\miniconda3\\envs\\esrg\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "output_dt16 = prototype(30, ([('dt16', 'dt16', 'dt16')]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0.15081637114302943',\n",
       "  '0.059826081390577976',\n",
       "  '-0.4620720682274576',\n",
       "  '-0.276165537277217',\n",
       "  '-0.17028688337596423'],\n",
       " ['0.17914571948116628',\n",
       "  '-0.43376318321814644',\n",
       "  '-0.46138133349382177',\n",
       "  '-0.3085013703668937',\n",
       "  '-0.30331289750296825'],\n",
       " ['0.17657737470448895',\n",
       "  '-0.4201452801977953',\n",
       "  '-0.46160257748393535',\n",
       "  '-0.18134103521055042',\n",
       "  '-0.007280742254338971'],\n",
       " ['0.1788830372971542',\n",
       "  '-0.3998560123808238',\n",
       "  '-0.489439786769501',\n",
       "  '-0.29380859354074434',\n",
       "  '-0.30234126470866285'],\n",
       " ['0.17852834831154546',\n",
       "  '-0.38572235228189317',\n",
       "  '-0.49501443268173867',\n",
       "  '-0.3011375837129916',\n",
       "  '-0.386056297957872'],\n",
       " ['0.1793671801620258',\n",
       "  '-0.4335617659972259',\n",
       "  '-0.5029713969536185',\n",
       "  '-0.2952394955767403',\n",
       "  '-0.2948780700571947'],\n",
       " ['0.17848053081075577',\n",
       "  '-0.3821636357536901',\n",
       "  '-0.5000401378831645',\n",
       "  '-0.3078186734491865',\n",
       "  '-0.3658817494918784'],\n",
       " ['0.17641447914611616',\n",
       "  '-0.34285307475515164',\n",
       "  '-0.3994069894984872',\n",
       "  '-0.23670340860939695',\n",
       "  '-0.3671198859290685'],\n",
       " ['0.1783113538608788',\n",
       "  '-0.3853407888305528',\n",
       "  '-0.4337613646929737',\n",
       "  '-0.30537563469924073',\n",
       "  '-0.35642330280263107'],\n",
       " ['-12.470942310400751',\n",
       "  '5.996665513665269',\n",
       "  '16.07625548397819',\n",
       "  '5.579407864168465',\n",
       "  '10.86262789984619'],\n",
       " ['0.17919352331033944',\n",
       "  '-0.4247468117136569',\n",
       "  '-0.47024573810039844',\n",
       "  '-0.3030057254310516',\n",
       "  '-0.380467760694468'],\n",
       " ['0.1786971750994283',\n",
       "  '-0.42915121161969666',\n",
       "  '-0.4253851247759062',\n",
       "  '-0.2732597102229613',\n",
       "  '-0.28042665443328024'],\n",
       " ['0.17860989499506333',\n",
       "  '-0.37922407097905686',\n",
       "  '-0.5098703901079497',\n",
       "  '-0.30792319564253096',\n",
       "  '-0.3806996376822809'],\n",
       " ['0.10927192016551007',\n",
       "  '-0.5006645461618475',\n",
       "  '0.4766120932471342',\n",
       "  '-0.04449139260901275',\n",
       "  '-0.42157771513036196'],\n",
       " ['0.1797241447444998',\n",
       "  '-0.43381432338704656',\n",
       "  '-0.482508082013758',\n",
       "  '-0.28871439856547504',\n",
       "  '-0.3821455500206264'],\n",
       " ['0.17918189382390243',\n",
       "  '-0.46568980994900283',\n",
       "  '-0.5764246119547118',\n",
       "  '-0.31189424472306726',\n",
       "  '-0.3892577524198407'],\n",
       " ['0.17706119039603388',\n",
       "  '-0.41273082056391747',\n",
       "  '-0.5345560046149612',\n",
       "  '-0.2408043314873142',\n",
       "  '-0.3702326957070205'],\n",
       " ['0.17475276218552696',\n",
       "  '-0.4555309029303412',\n",
       "  '-0.3989303508354585',\n",
       "  '-0.17209523750247038',\n",
       "  '0.12499424399141582'],\n",
       " ['0.17751921197150528',\n",
       "  '-0.42746801134729184',\n",
       "  '-0.3638513428188342',\n",
       "  '-0.2051487544498595',\n",
       "  '-0.3650721606105008'],\n",
       " ['0.1775901451384216',\n",
       "  '-0.3791620425115007',\n",
       "  '-0.4465998202509347',\n",
       "  '-0.24534032697400945',\n",
       "  '-0.37684322304524753'],\n",
       " ['-4.701160050168433',\n",
       "  '7.381162413258869',\n",
       "  '4.852835434272718',\n",
       "  '1.4491769572625823',\n",
       "  '0.4376566541788143'],\n",
       " ['0.17840994207037136',\n",
       "  '-0.3786923474338046',\n",
       "  '-0.4417018911429636',\n",
       "  '-0.3100391602510744',\n",
       "  '-0.3332643930965992'],\n",
       " ['0.17902484682174918',\n",
       "  '-0.45362858833305625',\n",
       "  '-0.39699481071090775',\n",
       "  '-0.2902192301421839',\n",
       "  '-0.2887273228965664'],\n",
       " ['0.16982800421505517',\n",
       "  '-0.464750938241857',\n",
       "  '-0.509082067218075',\n",
       "  '-0.30156941343215304',\n",
       "  '-0.3905330223170353'],\n",
       " ['0.04349993979272944',\n",
       "  '0.672764127470633',\n",
       "  '0.3461778753563597',\n",
       "  '3.247078990253557',\n",
       "  '-0.2823239259874375'],\n",
       " ['0.05562334930855796',\n",
       "  '0.6159116692552662',\n",
       "  '0.4061591418368474',\n",
       "  '0.1609331896290822',\n",
       "  '-0.11407600440936441'],\n",
       " ['0.1768766022110152',\n",
       "  '-0.457841454080037',\n",
       "  '-0.5213888674198685',\n",
       "  '-0.2775955993673566',\n",
       "  '-0.38874143812001033'],\n",
       " ['0.14287413657042825',\n",
       "  '-0.4696359185358339',\n",
       "  '-0.4493074662786664',\n",
       "  '-0.3021384740259642',\n",
       "  '-0.38289846508741193'],\n",
       " ['0.17238930793212995',\n",
       "  '-0.4897851396853524',\n",
       "  '-0.47681793570452247',\n",
       "  '-0.2570700546505947',\n",
       "  '-0.390099354246235'],\n",
       " ['0.17010973510449592',\n",
       "  '-0.3559985773058515',\n",
       "  '-0.5044154940621909',\n",
       "  '-0.29511472579957665',\n",
       "  '-0.389775452942516']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "def img_num_from_name(img_name):          # obtaining the image number from image name\n",
    "    ext = []\n",
    "    for i in range(len(img_name)):\n",
    "        if i > 14 and i < 23:\n",
    "            ext.append(img_name[i])\n",
    "    j = 0\n",
    "    for i in range(len(ext)):\n",
    "        if ext[i] == '0':\n",
    "            j = j + 1\n",
    "        if ext[i] != '0':\n",
    "            break\n",
    "    rightNum = []\n",
    "    for i in range(j, len(ext)):\n",
    "        rightNum.append(ext[i])\n",
    "    rightNum = ''.join(rightNum)\n",
    "    rightNum = int(rightNum)\n",
    "    return rightNum\n",
    "\n",
    "predicted_models_dt16 = []                                    # models predicted by dt16 premodel            \n",
    "for i in range(10):\n",
    "    for j in range(len(output_dt16[5][i][1][0])):\n",
    "        img_paths = (output_dt16[5][i][1][0][j][0])\n",
    "        img_nums = (img_num_from_name(img_paths))\n",
    "        if output_dt16[5][i][1][0][j][4] == 'tf-mobilenet_v1' and output_dt16[5][i][1][0][j][3] == 1:\n",
    "            predicted_models_dt16.append([img_nums, 'mobilenet_v1'])\n",
    "        elif output_dt16[5][i][1][0][j][4] == 'tf-inception_v4' and output_dt16[5][i][1][0][j][3] == 2:\n",
    "            predicted_models_dt16.append([img_nums, 'inception_v4'])\n",
    "        elif output_dt16[5][i][1][0][j][4] == 'tf-resnet_v1_152' and output_dt16[5][i][1][0][j][3] == 3:\n",
    "            predicted_models_dt16.append([img_nums, 'resnet_v1_152'])\n",
    "        elif output_dt16[5][i][1][0][j][4] == 'failed':\n",
    "            predicted_models_dt16.append([img_nums, 'failed'])\n",
    "print(len(predicted_models_dt16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'mobilenet_v1'],\n",
       " [16, 'mobilenet_v1'],\n",
       " [11, 'failed'],\n",
       " [21, 'mobilenet_v1'],\n",
       " [25, 'mobilenet_v1'],\n",
       " [58, 'mobilenet_v1'],\n",
       " [68, 'mobilenet_v1'],\n",
       " [83, 'mobilenet_v1'],\n",
       " [84, 'mobilenet_v1'],\n",
       " [87, 'mobilenet_v1'],\n",
       " [88, 'mobilenet_v1'],\n",
       " [90, 'mobilenet_v1'],\n",
       " [100, 'mobilenet_v1'],\n",
       " [126, 'mobilenet_v1'],\n",
       " [127, 'mobilenet_v1'],\n",
       " [133, 'mobilenet_v1'],\n",
       " [156, 'mobilenet_v1'],\n",
       " [167, 'mobilenet_v1'],\n",
       " [182, 'mobilenet_v1'],\n",
       " [195, 'mobilenet_v1'],\n",
       " [199, 'mobilenet_v1'],\n",
       " [200, 'mobilenet_v1'],\n",
       " [206, 'mobilenet_v1'],\n",
       " [218, 'mobilenet_v1'],\n",
       " [225, 'mobilenet_v1'],\n",
       " [231, 'mobilenet_v1'],\n",
       " [500, 'mobilenet_v1'],\n",
       " [520, 'mobilenet_v1'],\n",
       " [740, 'mobilenet_v1'],\n",
       " [746, 'mobilenet_v1']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_models_dt16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT16 premodel accuracy = 0.8666666666666667\n",
      "DT16 premodel average inference time = 215.07778551724135\n"
     ]
    }
   ],
   "source": [
    "import database\n",
    "n = 1\n",
    "accuracy_premodel = 0\n",
    "time_premodel = 0\n",
    "times = 0\n",
    "dt16_premodel_overhead = 43.00882                                           # average dt16 premodel overhead\n",
    "\n",
    "for i in range(len(predicted_models_dt16)):\n",
    "    if predicted_models_dt16[i][1] != 'failed':\n",
    "        times = times + 1                           # get_model_top_n (next line), where n is 1 or 5\n",
    "        accuracy_premodel = accuracy_premodel + (database.get_model_top_n(\"inference\", predicted_models_dt16[i][0], predicted_models_dt16[i][1], n))\n",
    "        time_premodel = time_premodel + (database.get_model_time(\"inference\", predicted_models_dt16[i][0], predicted_models_dt16[i][1]))\n",
    "\n",
    "dt16_accuracy = accuracy_premodel/len(predicted_models_dt16)                        # dt16 premodel accuracy\n",
    "dt16_time = (time_premodel/times) + dt16_premodel_overhead                             # dt16 premodel inference time\n",
    "print(\"DT16 premodel accuracy = {}\".format(dt16_accuracy))\n",
    "print(\"DT16 premodel average inference time = {}\".format(dt16_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x150df2488d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_names = ['DT16']\n",
    "accuracy = [dt16_accuracy]\n",
    "\n",
    "#model_names = ['Mobilenet', 'Inception', 'Resnet', 'LR', 'KNN', 'DT16', 'NB']\n",
    "# accuracy = [mobilenet_right/len(img_nums), inception_right/len(img_nums),\n",
    "#             resnet_right/len(img_nums), log_reg_accuracy, knn_accuracy, dt16_accuracy, nb_accuracy]\n",
    "\n",
    "for i in range(len(accuracy)):\n",
    "    accuracy[i] = accuracy[i]*100\n",
    "\n",
    "ypos = np.arange(len(model_names))\n",
    "\n",
    "plt.xticks(ypos, model_names)\n",
    "plt.ylabel(\"accuracy (%)\")\n",
    "plt.title(\"{} accuracy\".format(\"Top-1\"))\n",
    "plt.bar(ypos, accuracy)\n",
    "plt.ylim(top = 100)\n",
    "plt.ylim(bottom = 20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x150df580a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEICAYAAABF82P+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZg0lEQVR4nO3de5hWZb3/8feHEYREtGBE5CBYlKKiIp5NPKSIB0BNwUzdGiLXTt3tvb0UahdqP1NLf6WlISrb7YHckmJYJJi6Qwy2QJEKSCDHEQ0hTZDj4Hf/8ayxh3HNzJphFjPMfF7XNRfPWuu+1/N9UOfjute670cRgZmZWWUtGroAMzNrnBwQZmaWygFhZmapHBBmZpbKAWFmZqkcEGZmlsoBYc2CpI6SpklaJ+muhq6nvkiaJ+nkhq7DmqbdGroAs7qStAwYFhG/y9B8OLAGaBe76OQfSQ8DZRHxHxX7IuLghqvImjpfQVhzsT8wvy7hIMn/I2XNkgPCmgRJ/yRpuqQ7Jb0vaamkAcmxh4HLgRskrZf0FUktJI2U9JaktZKelPS5pH13SSHpG5JWAC8m+6+UtCA5/xRJ+xe9f0gaIWlRcvxeSSo6flXSd52k+ZL6JPv3k/SUpPeSmq+r4vMNBy4p+gzPJvuXSfpK8vomSRMkPZa8z+uSvihplKTVklZKOqPonHtJekjSO5LelvT/JJXU5z8X27U5IKwpOQZYCHQAfgg8JEkR8U/A48API6JtMiR1HTAY6AfsB7wP3FvpfP2Ag4D+kgYD3wbOB0qBl4FfVGp/DnAUcBhwEdAfQNKFwE3AZUA7YCCwVlIL4Fngz0Bn4DTgW5L6V/5gETG20mc4t4q/g3OBR4HPAn8CplD477wzcAtwf1Hb/wLKgS8ARwBnAMOqOK81Qw4Ia0qWR8QDEbGNwi+/TkDHKtpeDXwnIsoiYjOFX+BfrTScdFNEfBQRG5P2t0XEgogoB34AHF58FQHcHhEfRMQK4CXg8GT/MAq/2GdFweKIWE4hTEoj4paI2BIRS4AHgKE78HfwckRMSWqcQCHMbo+IrcATQHdJe0vqCAwAvpV8xtXAj3fwva2J8diqNSXvVryIiA3JCE/bKtruD0yU9HHRvm1sHygrK7W/u9ITUKLwf+bLK78/sKHovbsCb1VRw36SPijaV0Lh6qSu/lr0eiOwJgnMim2SuvYDWgLvFI2EtWD7z2zNnAPCmquVwJUR8UrlA5K6Jy+jUvtbI+LxOr7X56vYvzQiemY8T30+fbUS2Ax0SK42zD7FQ0zWXI0Bbq0YIpJUKmlQDe1HSTo4ab9Xcm8hiweB6yUdqYIvJO/7KvChpBsltZFUIukQSUdVcZ6/AgdkfM9qRcQ7wFTgLkntkpv2n5fUrz7Ob02DA8Kaq7uBScBUSeuAmRRucqeKiInAHcATkj4E3qAwhl+jiJgA3AqMB9YBzwCfS4Z+zqVwr2IphXkaDwJ7VXGqh4Bekj6Q9EyW967BZUArYD6Fm/S/pHDfxgwA7aJzhszMLGe+gjAzs1QOCDMzS+WAMDOzVA4IMzNL1aTmQXTo0CG6d+/e0GWYme0y5syZsyYiStOONamA6N69O7Nnz27oMszMdhmSlld1zENMZmaWygFhZmapHBBmZpaqSd2DMLOmZ+vWrZSVlbFp06aGLmWX1rp1a7p06ULLli0z93FAmFmjVlZWxp577kn37t0pWprcaiEiWLt2LWVlZfTo0SNzPw8xmVmjtmnTJtq3b+9w2AGSaN++fa2vwhwQZtboORx2XF3+Dh0QZmaWyvcgzGyX0n3kb+r1fMtuP7vGNm3btmX9+vXVtnn55ZcZMWIELVu2ZMaMGbRp06a+SmwwDohEff9LZ2b144GBndha9kFu538tw7k/jprb3TN2HEOu/GcGD7mERWs3U/hG16pFBBFBixY7PpDTu8veO3yONB5iMjPLaNaM6XzjwnP496svZ9DJRzPq2quICJ7+xSNMffYZ7r/7h4y69ioAHh5zD187+1S+evoJ3HfXbQC8vXIFg085hlu//e8MGdCPd1eVVdvu5hv+hfNOO46rv3Y+mzZuBGDF0iUMv3gwF55xIkMG9GPlsqUA/OhHP+Koo46id+/ejB49ul4+rwPCzKwW3pz3Gjfc9AMmvjiTshXL+dOsmZx/8WWcfPoA/u07t3DbTx/gD79/kRVLl/D4r1/gySkvM//1ucyZ+QoAy95axLlfHcqTz01j2VuLq2y3YulbDLl8GBNfmEG7vfbid7+dBMCo64Yz5LJhTJg6nUcmTqFDx45MnTqVRYsW8eqrrzJ37lzmzJnDtGnTdvizeojJzKwWDjn8SDp26gzAl3odwqqyFfQ5+rjt2syY9hIzpr3IkDNPAmDDRx+xfNkS9u3clU5dutK7z1E1tuvcdX8OPPhQAA469DBWrVzJR+vXsfrddzhtwDkA7N66NQATpk5l6tSpHHHEEQCsX7+eRYsWcdJJJ+3QZ3VAmJnVQstWrT553aKkhG3l2z7VJiK48pv/yoVfv2K7/W+vXEGbNp/J1K74fUpalLB52yYiIrWmiGDUqFFcffXVdfpMVfEQk5lZPTu+36k889+Ps+GjwpNPf31nFWvXvFfndhXa7tmOjp3248XnCg/VbNm8mY0bN9C/f3/GjRv3yZNWb7/9NqtXr97hz+ErCDPbpUy65oSGLqFGx/c7laWL/8Klg84A4DN7tOUHd99Pi5KSOrUrduvdY/j+yH/lvrt+wG4tW3Lnzx/mrDPOYMGCBRx3XGGoq23btjz22GPss88+O/Q5VNUly66ob9++UdcvDPJjrmaN0wMDO9Gx2wENXUajlvUx1wULFnDQQQdtt0/SnIjom9beQ0xmZpbKAWFmZqkcEGZmlirXgJB0pqSFkhZLGply/BJJryU/f5B0WNa+ZmaWr9wCQlIJcC8wAOgFXCypV6VmS4F+EdEb+D4wthZ9zcwsR3leQRwNLI6IJRGxBXgCGFTcICL+EBHvJ5szgS5Z+5qZWb7ynAfRGVhZtF0GHFNN+28Av61tX0nDgeEA3bp1q2utZraL6P3g/vV6vteGLa+xzRH7t6fngb0oLy/ngJ5f4vs/vm+7GdE707Ff6sLMhWXVtsmyPHkWeV5BpH19UeqkC0mnUAiIG2vbNyLGRkTfiOhbWlpap0LNzKqze+s2PDnlZZ5+YQYtW7ZkwqP/ud3xbds+vdxGU5DnFUQZ0LVouwuwqnIjSb2BB4EBEbG2Nn3NzHa2I44+jkUL5jFrxnTG/PgOSvfpyML5b/DL51/h7ttuYvaMV9iyZTNDLh/GhV+/glkzpvPzu26jfek+vDnvdU4bcA49D+zF4w/dz+ZNG/nJg4/TtXsPVpWtYPT11/L+2jV8tn0HbrnrZ3Tq3JWyFcsZde1VbCsv5/iTT9uulofH3MPUZ5+hRZRz3nnncfPNN9frZ83zCmIW0FNSD0mtgKHApOIGkroBTwOXRsRfatPXzGxnKy8v55WXfkfPAwvPzLwx949cc8N3mfjiTCY+8Sht99yL8b95kfG/fpGnxz9C2YrC8NVfFrzBDTfdxlPPv8Kvn3qS5UveYvyvX+D8iy/jFw+PBeC2797AuRcM5ZfPv8JZgy/kju8VHt784eiRXHTplYz/zYt0KP3H0hnFS4rX5xLfxXILiIgoB64BpgALgCcjYp6kEZJGJM2+B7QH7pM0V9Ls6vrmVauZWXU2b9rIRf2/zNfOPoV9O3fhvKGXAnDI4X3o0q1wT2TGtJd49qknuKj/l/n6wK/wwQd/Y8XStwA4+LA+lHbcl1a7707X/btz3EmnAPCFA3uxauUKAF6bM4sBg78KwDkXDOFPs2YCMHf2/3LmoAs+2V+heKnwPn368Oabb7Jo0aJ6/dy5LtYXEZOByZX2jSl6PQwYlrWvmVlDqLgHUVmbz2y/dPfIW+7ghErDQLNmTN9+ifAWLWjVavdPXpdvK099T0mpr4vfr2KpcH/lqJlZI3Z8v1OZ8Og4tm7dCsCyJYvZsOGjzP0PO/Jonpv0FACTJ07g8KOOBeDwvsdst7/4/YqXCq+vJb6LeblvM9ulZHkstSGcf/FlrFq5gqED+hERfLZ9B37y4GOZ+994yx2Mvv4a/mvMTz+5SQ1ww823M+raqxj/0P2cdta5n7QvXiq8dcuSelviu5iX+054uW+zxsnLfdfMy32bmdlO5YAwM7NUDggza9SCoCkNhTeUuvwdOiDMrFFb/sFWyjd86JDYARHB2rVrad26da36+SkmM2vUfvq/73MtsP/ea1DqMm22YF2bGtu0bt2aLl261NiumAPCzBq1Dzd/zK3T1tbcsBlbdvvZuZzXQ0xmZpbKAWFmZqkcEGZmlsoBYWZmqRwQZmaWygFhZmapHBBmZpbKAWFmZqkcEGZmlsoBYWZmqRwQZmaWygFhZmapHBBmZpbKAWFmZqkcEGZmlsoBYWZmqRwQZmaWygFhZmapHBBmZpbKAWFmZqkcEGZmlsoBYWZmqRwQZmaWygFhZmapHBBmZpbKAWFmZqkcEGZmlmq3mhpIOg74OvBloBOwEXgD+A3wWET8PdcKzcysQVR7BSHpt8AwYApwJoWA6AX8B9Aa+JWkgXkXaWZmO19NVxCXRsSaSvvWA39Mfu6S1CGXyszMrEFVewVREQ6S9pDUInn9RUkDJbUsbpNG0pmSFkpaLGlkyvEDJc2QtFnS9ZWOLZP0uqS5kmbX5cOZmVnd1XgPIjEN+LKkzwIvALOBIcAlVXWQVALcC5wOlAGzJE2KiPlFzf4GXAcMruI0p1QXQGZmlp+sTzEpIjYA5wM/jYjzKNyLqM7RwOKIWBIRW4AngEHFDSJidUTMArbWsm4zM8tZ5oBInma6hMLTS1Dz1UdnYGXRdlmyL6sApkqaI2l4NYUNlzRb0uz33nuvFqc3M7PqZA2IfwFGARMjYp6kA4CXauijlH1Ri9pOiIg+wADgm5JOSmsUEWMjom9E9C0tLa3F6c3MrDqZ7kFExDQK9yEqtpdQuHdQnTKga9F2F2BV1sIiYlXy52pJEykMWU2rvpeZmdWXmuZBjJV0aBXH9pB0paSqblTPAnpK6iGpFTAUmJSlqOTce1a8Bs6gMDnPzMx2kpquIO4DvpuExBvAexQmyPUE2gHjgMfTOkZEuaRrKEyyKwHGJcNTI5LjYyTtS+GJqHbAx5K+ReHmdwdgoqSKGsdHxHM78kHNzKx2qg2IiJgLXCSpLdCXfyy1sSAiFtZ08oiYDEyutG9M0et3KQw9VfYhcFhN5zczs/xkvQexHviffEsxM7PGxKu5mplZKgeEmZmlqlVAJE8UmZlZM5ApICQdL2k+sCDZPkzSfblWZmZmDSrrFcSPgf7AWoCI+DOQOrPZzMyahsxDTBGxstKubfVci5mZNSJZl/teKel4IJJZ0deRDDeZmVnTlPUKYgTwTQqrsZYBhyfbZmbWRGWdKLeGar4cyMzMmp5MASGpB3At0L24T0QMzKcsMzNraFnvQTwDPAQ8C3ycWzVmZtZoZA2ITRFxT66VmJlZo5I1IO6WNBqYCmyu2BkRf8ylKjMza3BZA+JQ4FLgVP4xxBTJtpmZNUFZA+I84ICI2JJnMWZm1nhknQfxZ2DvHOswM7NGJusVREfgTUmz2P4ehB9zNTNrorIGxOhcqzAzs0Yn60zq3+ddiJmZNS7VBoSk6RFxoqR1FJ5a+uQQEBHRLtfqzMyswVQbEBFxYvLnnjunHDMzayyyfqPco1n2mZlZ05H1MdeDizck7QYcWf/lmJlZY1FtQEgaldx/6C3pw+RnHfBX4Fc7pUIzM2sQ1QZERNyW3H/4UUS0S372jIj2ETFqJ9VoZmYNINMQk8PAzKz5yXoPwszMmhkHhJmZpcocEJJOlHRF8ro0+RpSMzNrorLOgxgN3AhU3ItoCTyWV1FmZtbwsl5BnAcMBD4CiIhVgGdXm5k1YVkDYktEBMl6TJL2yK8kMzNrDLIGxJOS7gf2lnQV8DvggfzKMjOzhpZ1ue87JZ0OfAh8CfheRDyfa2VmZtagMgVE8sTSyxWhIKmNpO4RsSzP4szMrOFkHWKaAHxctL0t2WdmZk1U1oDYLSK2VGwkr1vlU5KZmTUGWQPiPUkDKzYkDQLW5FOSmZk1BlkDYgTwbUkrJK2kMGnu6po6STpT0kJJiyWNTDl+oKQZkjZLur42fc3MLF9Zn2J6CzhWUltAEbGupj6SSoB7gdOBMmCWpEkRMb+o2d+A64DBdehrZmY5yvoU0+7ABUB3YDdJAETELdV0OxpYHBFLknM8AQwCPvklHxGrgdWSzq5tXzMzy1fWIaZfUfgFXU5huY2Kn+p0BlYWbZcl+7LYkb5mZlYPMl1BAF0i4sxanlsp+6K++0oaDgwH6NatW8bTm5lZTbJeQfxB0qG1PHcZ0LVouwuwqr77RsTYiOgbEX1LS0trWaKZmVUla0CcCMxJnip6TdLrkl6roc8soKekHpJaAUOBSRnfb0f6mplZPcg6xDSgtieOiHJJ1wBTgBJgXETMkzQiOT5G0r7AbKAd8LGkbwG9IuLDtL61rcHMzOou62OuyyWdCPSMiP+UVAq0zdBvMjC50r4xRa/fpTB8lKmvmZntPP5GOTMzS+VvlDMzs1T+RjkzM0vlb5QzM7NUNd6kVmFdjf8GDsTfKGdm1mzUGBAREZKeiYgjAYeCmVkzkXWIaaako3KtxMzMGpWsE+VOAUZIWkbhSSZRuLjonVdhZmbWsHKbSW1mZru2TENMEbGcwuJ5pyavN2Tta2ZmuybPpDYzs1SeSW1mZqk8k9rMzFJ5JrWZmaWq9ikmSbtHxOaIuFPS6XgmtZlZs1HTY64zgD6SHo2IS/FMajOzZqOmgGgl6XLgeEnnVz4YEU/nU5aZmTW0mgJiBHAJsDdwbqVjATggzMyaqGoDIiKmA9MlzY6Ih3ZSTWZm1ghk/U7qhyQdD3Qv7hMRj+RUl5mZNbBMASHpUeDzwFxgW7I7AAeEmVkTlXWxvr5Ar2SynJmZNQNZJ8q9AeybZyFmZta4ZL2C6ADMl/QqsLliZ0QMzKUqMzNrcFkD4qY8izAzs8Yn61NMv8+7EDMza1xqWotpekScKGkdyUquFYcofOVou1yrMzOzBlPTRLkTkz/93Q9mZs2MvzbUzMxSOSDMzCyVA8LMzFI5IMzMLJUDwszMUjkgzMwslQPCzMxSOSDMzCyVA8LMzFI5IMzMLJUDwszMUuUaEJLOlLRQ0mJJI1OOS9I9yfHXJPUpOrZM0uuS5kqanWedZmb2aVm/D6LWJJUA9wKnA2XALEmTImJ+UbMBQM/k5xjg58mfFU6JiDV51WhmZlXL8wriaGBxRCyJiC3AE8CgSm0GAY9EwUxgb0mdcqzJzMwyyjMgOgMri7bLkn1Z2wQwVdIcScNzq9LMzFLlNsRE4UuFKotatDkhIlZJ2gd4XtKbETHtU29SCI/hAN26dduRes3MrEieVxBlQNei7S7AqqxtIqLiz9XARApDVp8SEWMjom9E9C0tLa2n0s3MLM+AmAX0lNRDUitgKDCpUptJwGXJ00zHAn+PiHck7SFpTwBJewBnAG/kWKuZmVWS2xBTRJRLugaYApQA4yJinqQRyfExwGTgLGAxsAG4IuneEZgoqaLG8RHxXF61mpnZp+V5D4KImEwhBIr3jSl6HcA3U/otAQ7LszYzM6ueZ1KbmVmqXK8gdiXLWn+toUswM6ujv+dyVl9BmJlZKgeEmZmlckCYmVkqB4SZmaVyQJiZWSoHhJmZpXJAmJlZKgeEmZmlckCYmVkqB4SZmaVyQJiZWSoHhJmZpXJAmJlZKgeEmZmlckCYmVkqB4SZmaVyQJiZWSoHhJmZpXJAmJlZKgeEmZmlckCYmVkqB4SZmaVyQJiZWSoHhJmZpXJAmJlZKgeEmZmlckCYmVkqB4SZmaVyQJiZWSoHhJmZpXJAmJlZKgeEmZmlckCYmVkqB4SZmaVyQJiZWSoHhJmZpXJAmJlZKgeEmZmlyjUgJJ0paaGkxZJGphyXpHuS469J6pO1r5mZ5Su3gJBUAtwLDAB6ARdL6lWp2QCgZ/IzHPh5LfqamVmO8ryCOBpYHBFLImIL8AQwqFKbQcAjUTAT2FtSp4x9zcwsR7vleO7OwMqi7TLgmAxtOmfsC4Ck4RSuPgDWS1q4AzWb5aUDsKahi7Am6mbtSO/9qzqQZ0CkVRwZ22TpW9gZMRYYW7vSzHYuSbMjom9D12FWG3kGRBnQtWi7C7AqY5tWGfqamVmO8rwHMQvoKamHpFbAUGBSpTaTgMuSp5mOBf4eEe9k7GtmZjnK7QoiIsolXQNMAUqAcRExT9KI5PgYYDJwFrAY2ABcUV3fvGo12wk8DGq7HEWkDu2bmVkz55nUZmaWygFhZmapHBBmdSBpm6S5kuZJ+rOkf5PUQlL/ZP9cSeuT5WLmSnpEUntJLyX7f1bpfK0kjZX0F0lvSrqgoT6bWYU8H3M1a8o2RsThAJL2AcYDe0XEaAoPVyDpf4DrI2J2sr0H8F3gkOSn2HeA1RHxRUktgM/tjA9hVh1fQZjtoIhYTWE2/zWSqpzSGhEfRcR0YFPK4SuB25J2H0eEZ11bg3NAmNWDiFhC4b+nfWrbV9LeycvvS/qjpAmSOtZnfWZ14YAwqz91XRBnNwqrBbwSEX2AGcCd9VaVWR05IMzqgaQDgG3A6jp0X0thoujEZHsC0Kfq5mY7hwPCbAdJKgXGAD+LOsw8Tfo8C5yc7DoNmF9vBZrVkZ9iMqubNpLmAi2BcuBR4P/X1EnSMqAd0ErSYOCMiJgP3Ag8KuknwHsky86YNSQvtWFmZqk8xGRmZqkcEGZmlsoBYWZmqRwQZmaWygFhZmapHBBmZpbKAWFmZqn+Dz2oTZTvyjSUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "times = [dt16_time]\n",
    "times_premodel = [dt16_premodel_overhead]\n",
    "\n",
    "# times = [mobilenet_time/len(img_nums), inception_time/len(img_nums),\n",
    "#          resnet_time/len(img_nums), log_reg_time, knn_time, dt16_time, nb_time]\n",
    "# times_premodel = [0, 0, 0, log_reg_premodel_overhead, knn_premodel_overhead, dt16_premodel_overhead, nb_premodel_overhead]\n",
    "\n",
    "for i in range(len(times)):\n",
    "    times[i] = times[i]/1000\n",
    "\n",
    "for i in range(len(times_premodel)):\n",
    "    times_premodel[i] = times_premodel[i]/1000\n",
    "    \n",
    "plt.xticks(ypos, model_names)\n",
    "plt.ylabel(\"inference time (s)\")\n",
    "plt.title(\"Inference time\")\n",
    "plt.bar(ypos, times, label = \"Inference\")\n",
    "plt.bar(ypos, times_premodel, label = \"Premodel\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esrg",
   "language": "python",
   "name": "esrg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
